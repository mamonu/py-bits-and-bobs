{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 02_MissingData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using `where`, `isNull`, `dropna`, `fillna`, `isnan` to find/fill  missing data on Pyspark dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "spark = SparkSession.builder.appName('PySparkmissing').getOrCreate()\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- ip_address: string (nullable = true)\n",
      " |-- SPAM: integer (nullable = true)\n",
      " |-- numvar: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spdfA = spark.read.csv('02_MockDataset.csv',inferSchema=True,header=True)\n",
    "spdfA.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_nullsnan(df):\n",
    "    null_counts = []          \n",
    "    for col in df.dtypes:     #iterate through the column data types \n",
    "        cname = col[0]        #column name   \n",
    "        ctype = col[1]        #column type\n",
    "        if ctype != 'string': #no processing of string columns (can't have nulls or nan's)\n",
    "            nulls = df.where( df[cname].isNull() | isnan(df[cname]) ).count()\n",
    "            result = tuple([cname, nulls])  #(column name, null count)\n",
    "            null_counts.append(result)      #append tuple in our output list\n",
    "    return null_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('id', 0), ('SPAM', 7), ('numvar', 7)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nullcount = count_nullsnan(spdfA)\n",
    "nullcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spdfA.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Dropping Null Values\n",
    "\n",
    "There are three things we can do with our null values now that we know what's in our dataframe. \n",
    "\n",
    "We can ignore them, we can drop them, or we can replace them. \n",
    "\n",
    "Remember, pySpark dataframes are immutable, so we can't actually change the original dataset. \n",
    "\n",
    "All operations return an entirely new dataframe, though we can tell it to overwrite the existing one with \n",
    "\n",
    "`df = df.some_operation()` which ends up functionaly equivalent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The df.dropna() method has two arguments here:  how can equal `any` or `all` \n",
    "\n",
    "the first drops a row if any value in it is null, the second drops a row only if all values are.\n",
    "\n",
    "The subset argument takes a list of columns that you want to look in for null values. \n",
    "It does not actually subset the dataframe; it just checks in those three columns, \n",
    "then drops the row for the entire dataframe if that subset meets the criteria. \n",
    "This can be left off if it should check all columns for null.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_drops = spdfA.dropna(how='all', subset=['SPAM', 'numvar'])\n",
    "df_drops.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replacing Null Values\n",
    "\n",
    "The below line goes through all of columns 'SPAM' and 'numvar' and replaces null values with the value we specified,\n",
    "in this case a 0. \n",
    "\n",
    "To verify we re-run the command on our new dataframe to count nulls that we used above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('id', 0), ('SPAM', 0), ('numvar', 0)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fill = spdfA.fillna(0, subset=['SPAM','numvar'])\n",
    "\n",
    "count_nullsnan(df_fill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## TODO: imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
